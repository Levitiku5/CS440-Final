{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c5c38a-4aa9-4763-aff2-7743d1b339ef",
   "metadata": {},
   "source": [
    "Load the dataset and inspect properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a27f4c-a7bc-421b-aa4e-6ca45ecf5119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (3.1.0)\n",
      "Requirement already satisfied: torch in /usr/local/anaconda3/2022.08/lib/python3.9/site-packages (1.12.1)\n",
      "Requirement already satisfied: transformers in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (4.47.0)\n",
      "Requirement already satisfied: accelerate in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (1.1.1)\n",
      "Requirement already satisfied: evaluate in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (0.4.3)\n",
      "Collecting fsspec[http]<=2024.9.0,>=2023.1.0\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/anaconda3/2022.08/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Collecting tqdm>=4.66.3\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: filelock in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/anaconda3/2022.08/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/anaconda3/2022.08/lib/python3.9/site-packages (from datasets) (1.21.5)\n",
      "Collecting requests>=2.32.2\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: aiohttp in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: pandas in /usr/local/anaconda3/2022.08/lib/python3.9/site-packages (from datasets) (1.4.3)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/anaconda3/2022.08/lib/python3.9/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: psutil in /usr/local/anaconda3/2022.08/lib/python3.9/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/anaconda3/2022.08/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/anaconda3/2022.08/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/anaconda3/2022.08/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/anaconda3/2022.08/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/2022.08/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/2022.08/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/anaconda3/2022.08/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/anaconda3/2022.08/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/anaconda3/2022.08/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /s/bach/j/under/martel/.local/lib/python3.9/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Installing collected packages: tqdm, requests, fsspec\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/s/bach/j/under/martel/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.9.0 requests-2.32.3 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#!pip install datasets torch transformers accelerate evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5ed3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e731df",
   "metadata": {},
   "source": [
    "### Dataset metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11d73a77-36f3-4f61-aedd-dcec12fca62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/latest/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset_builder\n",
    "ds_builder = load_dataset_builder(\"google/code_x_glue_ct_code_to_text\", \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e3b2d32-8bf4-479c-be3b-a544ea7a75a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='int32', id=None),\n",
       " 'repo': Value(dtype='string', id=None),\n",
       " 'path': Value(dtype='string', id=None),\n",
       " 'func_name': Value(dtype='string', id=None),\n",
       " 'original_string': Value(dtype='string', id=None),\n",
       " 'language': Value(dtype='string', id=None),\n",
       " 'code': Value(dtype='string', id=None),\n",
       " 'code_tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'docstring': Value(dtype='string', id=None),\n",
       " 'docstring_tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'sha': Value(dtype='string', id=None),\n",
       " 'url': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_builder.info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33925a3d-d809-4812-b140-5b703c0112ea",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "Skip examples to make training time feasable on CSU dept machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a734008-a354-4965-acf9-23f370dd5bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 251820/251820 [00:09<00:00, 27704.92 examples/s]\n",
      "Generating validation split: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13914/13914 [00:00<00:00, 24658.34 examples/s]\n",
      "Generating test split: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14918/14918 [00:00<00:00, 24102.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"google/code_x_glue_ct_code_to_text\", \"python\")\n",
    "dataset['train'] = dataset['train'].skip(180000)\n",
    "dataset['test'] = dataset['test'].skip(7800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "684b0710-f3c4-4eda-8e23-ef4f99588353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (71820, 12), 'validation': (13914, 12), 'test': (7118, 12)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1411544b-84ee-44bd-9758-15e2b149ce99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['id',\n",
       "  'repo',\n",
       "  'path',\n",
       "  'func_name',\n",
       "  'original_string',\n",
       "  'language',\n",
       "  'code',\n",
       "  'code_tokens',\n",
       "  'docstring',\n",
       "  'docstring_tokens',\n",
       "  'sha',\n",
       "  'url'],\n",
       " 'validation': ['id',\n",
       "  'repo',\n",
       "  'path',\n",
       "  'func_name',\n",
       "  'original_string',\n",
       "  'language',\n",
       "  'code',\n",
       "  'code_tokens',\n",
       "  'docstring',\n",
       "  'docstring_tokens',\n",
       "  'sha',\n",
       "  'url'],\n",
       " 'test': ['id',\n",
       "  'repo',\n",
       "  'path',\n",
       "  'func_name',\n",
       "  'original_string',\n",
       "  'language',\n",
       "  'code',\n",
       "  'code_tokens',\n",
       "  'docstring',\n",
       "  'docstring_tokens',\n",
       "  'sha',\n",
       "  'url']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "152e71a9-5f32-4eda-9cfc-e9f971b12f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 180000,\n",
       " 'repo': 'Tanganelli/CoAPthon3',\n",
       " 'path': 'coapthon/messages/option.py',\n",
       " 'func_name': 'Option.length',\n",
       " 'original_string': 'def length(self):\\n        \"\"\"\\n        Return the value length\\n\\n        :rtype : int\\n        \"\"\"\\n        if isinstance(self._value, int):\\n            return byte_len(self._value)\\n        if self._value is None:\\n            return 0\\n        return len(self._value)',\n",
       " 'language': 'python',\n",
       " 'code': 'def length(self):\\n        \"\"\"\\n        Return the value length\\n\\n        :rtype : int\\n        \"\"\"\\n        if isinstance(self._value, int):\\n            return byte_len(self._value)\\n        if self._value is None:\\n            return 0\\n        return len(self._value)',\n",
       " 'code_tokens': ['def',\n",
       "  'length',\n",
       "  '(',\n",
       "  'self',\n",
       "  ')',\n",
       "  ':',\n",
       "  'if',\n",
       "  'isinstance',\n",
       "  '(',\n",
       "  'self',\n",
       "  '.',\n",
       "  '_value',\n",
       "  ',',\n",
       "  'int',\n",
       "  ')',\n",
       "  ':',\n",
       "  'return',\n",
       "  'byte_len',\n",
       "  '(',\n",
       "  'self',\n",
       "  '.',\n",
       "  '_value',\n",
       "  ')',\n",
       "  'if',\n",
       "  'self',\n",
       "  '.',\n",
       "  '_value',\n",
       "  'is',\n",
       "  'None',\n",
       "  ':',\n",
       "  'return',\n",
       "  '0',\n",
       "  'return',\n",
       "  'len',\n",
       "  '(',\n",
       "  'self',\n",
       "  '.',\n",
       "  '_value',\n",
       "  ')'],\n",
       " 'docstring': 'Return the value length\\n\\n        :rtype : int',\n",
       " 'docstring_tokens': ['Return', 'the', 'value', 'length'],\n",
       " 'sha': '985763bfe2eb9e00f49ec100c5b8877c2ed7d531',\n",
       " 'url': 'https://github.com/Tanganelli/CoAPthon3/blob/985763bfe2eb9e00f49ec100c5b8877c2ed7d531/coapthon/messages/option.py#L80-L90'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33365edb",
   "metadata": {},
   "source": [
    "## Create Tokenizer\n",
    "\n",
    "Roberta Tokenizer creates byte-level Byte-Pair-Encoding, suitable for our base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ced56d7-23ed-4162-8d61-8b2b2275aaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "2024-12-06 04:38:04.381554: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9c322",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "\n",
    "Prefix given to match one of the applications of the pretrained model and improve performance. \n",
    "code_tokens used instead of 'code' because the code_tokens do not contain the docstring that the model is trying to generate. The code tokens are instead combined into a string in this step suitable for the model's tokenization process. Use -100 for label on padding tokens so that they do not cause reduced model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71110ec1-3767-4c62-ae91-14f71ca86bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Summarize Python: \"\n",
    "def preprocess(samples):\n",
    "    codestrings = samples['code_tokens']\n",
    "    \n",
    "    \n",
    "    docstrings = samples['docstring']\n",
    "\n",
    "    inputs = []\n",
    "    for codestring in codestrings:\n",
    "        codestring = ' '.join(codestring)\n",
    "        inputs.append(prefix + codestring)\n",
    "        \n",
    "    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(docstrings, padding=\"max_length\", truncation=True).input_ids\n",
    "\n",
    "    labels_with_ignore_index = []\n",
    "    for labels_example in labels:\n",
    "        labels_example = [label if label != 0 else -100 for label in labels_example]\n",
    "        labels_with_ignore_index.append(labels_example)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels_with_ignore_index\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ecfeb3e-8549-4b83-9ea2-844208a17092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71820/71820 [00:47<00:00, 1512.51 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13914/13914 [00:10<00:00, 1322.24 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7118/7118 [00:04<00:00, 1472.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0113edd1",
   "metadata": {},
   "source": [
    "## Dataset loaders\n",
    "batch size is sized to not run out of memory on CSU dept machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "532eae99-d518-40f3-8208-5aa8a77853f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
    "train_dataloader = DataLoader(dataset['train'], shuffle=True, batch_size=16)\n",
    "valid_dataloader = DataLoader(dataset['validation'], batch_size=8)\n",
    "test_dataloader = DataLoader(dataset['test'], batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de53719",
   "metadata": {},
   "source": [
    "## Training columns\n",
    "attention_mask is included so that extra padding input_id's are not mistaken as actual input by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1204ed0d-0536-455a-b833-90bd4487cfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b2e64",
   "metadata": {},
   "source": [
    "## Example of decoding producing original docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30845957-cb94-4379-84fa-5ee98b6a0727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Remove a relationship from one user to another, with the same caveats\\n        and behavior as adding a relationship.</s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = batch['labels'][0]\n",
    "tokenizer.decode([label for label in labels if label != -100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3055f656",
   "metadata": {},
   "source": [
    "## Base pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ab2ff88-3212-41c1-8774-fe6f308cb6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e9552ae-03c5-478e-9796-3a670c086c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbfdcfa",
   "metadata": {},
   "source": [
    "## Hypr-parameters\n",
    "increasing these, particularly num_epochs results in extremely long training time on dept machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f6f78a0-fc68-4db1-b9ae-d5d1d29fce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf48a9f",
   "metadata": {},
   "source": [
    "## Backend processing device\n",
    "Ensure CUDA is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "671ded69-a650-42e7-ba93-5f7d772eb72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32100, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947b71a9",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b9f0701-5395-434d-86e4-9745ea5fd62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                          | 0/4489 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4489/4489 [40:11<00:00,  2.02it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51012cfa",
   "metadata": {},
   "source": [
    "## Save the trained model for future use and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7630e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \".\"\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b44e06f",
   "metadata": {},
   "source": [
    "## Example of loading in dataset and using model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e70c2714-ceb7-4069-8088-ca3496745c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url'],\n",
      "    num_rows: 13914\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"google/code_x_glue_ct_code_to_text\", \"python\")\n",
    "print(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f14f8cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code: ['def', 'wrap_deepmind', '(', 'env', ',', 'episode_life', '=', 'True', ',', 'clip_rewards', '=', 'True', ',', 'frame_stack', '=', 'False', ',', 'scale', '=', 'False', ')', ':', 'if', 'episode_life', ':', 'env', '=', 'EpisodicLifeEnv', '(', 'env', ')', 'if', \"'FIRE'\", 'in', 'env', '.', 'unwrapped', '.', 'get_action_meanings', '(', ')', ':', 'env', '=', 'FireResetEnv', '(', 'env', ')', 'env', '=', 'WarpFrame', '(', 'env', ')', 'if', 'scale', ':', 'env', '=', 'ScaledFloatFrame', '(', 'env', ')', 'if', 'clip_rewards', ':', 'env', '=', 'ClipRewardEnv', '(', 'env', ')', 'if', 'frame_stack', ':', 'env', '=', 'FrameStack', '(', 'env', ',', '4', ')', 'return', 'env']\n"
     ]
    }
   ],
   "source": [
    "test_example = dataset['validation'][32]\n",
    "print(\"Code:\", test_example['code_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd08e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d376b5",
   "metadata": {},
   "source": [
    "### Ensure that the sample input is the joined tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29de9d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated docstring: Wrap an environment into a deepmind environment.\n",
      "\n",
      "    :param env: The environment\n"
     ]
    }
   ],
   "source": [
    "test_ex = ' '.join(test_example['code_tokens'])\n",
    "input_ids = tokenizer(test_ex, return_tensors='pt').input_ids\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(\"Generated docstring:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78485e42-ae81-47b1-9a10-2cfe301fccb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True docstring:  Configure environment for DeepMind-style Atari.\n"
     ]
    }
   ],
   "source": [
    "print(\"True docstring: \", test_example['docstring'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0754057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Using cached rouge_score-0.1.2-py3-none-any.whl\n",
      "Collecting absl-py (from rouge_score)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nltk in /Users/kevinzhu/opt/anaconda3/lib/python3.9/site-packages (from rouge_score) (3.7)\n",
      "Requirement already satisfied: numpy in /Users/kevinzhu/opt/anaconda3/lib/python3.9/site-packages (from rouge_score) (1.20.3)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/kevinzhu/opt/anaconda3/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/kevinzhu/opt/anaconda3/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/kevinzhu/opt/anaconda3/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kevinzhu/opt/anaconda3/lib/python3.9/site-packages (from nltk->rouge_score) (2021.11.2)\n",
      "Requirement already satisfied: tqdm in /Users/kevinzhu/opt/anaconda3/lib/python3.9/site-packages (from nltk->rouge_score) (4.62.3)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Installing collected packages: absl-py, rouge_score\n",
      "Successfully installed absl-py-2.1.0 rouge_score-0.1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1390c88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.2857142857142857, recall=0.3333333333333333, fmeasure=0.30769230769230765), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.2857142857142857, recall=0.3333333333333333, fmeasure=0.30769230769230765)}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(\"Configure environment for DeepMind-style Atari.\",\n",
    "                      \"Wrap an environment into a deepmind environment.\")\n",
    "\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
