%        File: report.tex
%     Created: Tue Nov 12 05:00 PM 2024 M
% Last Change: Tue Nov 12 05:00 PM 2024 M
%

\documentclass[12pt]{article}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{amsmath}%
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{graphicx}
\usepackage{enumitem}
%-------------------------------------------
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\setlength{\textwidth}{7.0in}
\setlength{\oddsidemargin}{-0.35in}
\setlength{\topmargin}{-0.5in}
\setlength{\textheight}{9.0in}
\setlength{\parindent}{0.3in}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\U}{\mathbb{U}}

\usepackage{listings}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
backgroundcolor=\color{backcolour},   
commentstyle=\color{codegreen},
keywordstyle=\color{magenta},
numberstyle=\tiny\color{codegray},
stringstyle=\color{codepurple},
basicstyle=\ttfamily\footnotesize,
breakatwhitespace=false,         
breaklines=true,                 
captionpos=b,                    
keepspaces=true,                 
numbers=left,                    
numbersep=5pt,                  
showspaces=false,                
showstringspaces=false,
showtabs=false,                  
tabsize=2
}

\lstset{style=mystyle}

\begin{document}
	\title{ AI-Generated Python Code Documentation: A Case Study }
	\author{Levi Shafter, Ryan Martel, Kevin Zhu}
	\maketitle

	\section*{Abstract}
	\paragraph{} The field of artificial intelligence has seen massive strides forward in recent years. As our world relies more and more on software, the prospect of using artificial intelligence during the development process is a fascinating one with massive potential. We know of the utility that can be gained from large language models trained on supercomputers. However, if massive resources are typically required for useful model generation, exactly where is the floor when it comes to engineering something useful? 
	\paragraph{} The purpose of this paper is to demonstrate a proof-of-concept with building a specialized language model for generating Python code documentation. Our small team was able to generate a competent model using very limited resources. While certainly not on-par with the likes of ChatGPT-4, our model is still able to generate useful docstrings. It is possible that there is some niche where models must be quickly built on the fly, and our experiment has shown that there are possibilities for developing useful models where speed is preferred to accuracy. 

	\section{Introduction}
	\paragraph{} In an era of LLM-based revolution, many are exploring the endless possibilities of using artificial intelligence to aid in software development. This comes as no surprise considering the astounding need for software in nearly every aspect of modern industry. \cite{emerald2019csrImpact} With a dramatic increase in the need for software, some of the hottest topics being explored are the potential of enhancing the software development process, creating new tools as well as enhancing the old, and even automating certain aspects of writing code (See Section 3: Related Work). 
	\paragraph{} In this paper, we detail our experience with training a model using machine learning for the purpose of generating Python documentation. We discuss other similar works, compare our results, and discuss applications and potential improvements as well as viable future work. With so much recent progress in the field of AI \cite{shietal2020IoT_AI}, models like ours might drastically change the way we think about and approach software development.

	\section{Related Work}
	\paragraph{} Several recent studies and developments in the field of artificial intelligence and software development have highlighted significant advancements. These include the use of ML in resolving code review comments \cite{google2023codeReviewML}, the introduction of IDE plugins for AI-assisted development \cite{qodoAI2024idePlugin}, innovations in AI-powered fuzzing \cite{google2023aiFuzzing, cso2024aiFuzzing}, research into the out-of-distribution generalization of pre-trained language models \cite{chen2023oodGeneralization}, efforts in automated documentation with ChatGPT \cite{awekrx2024autodoc}, and the enhancement of large language models for multi-modal research synthesis \cite{wiggers2023largeModels}.


	\section{Method}
	For building our model, we used the following Python modules:
	\begin{itemize}
		\item datasets
		\item torch
		\item transformers
		\item accelerate
		\item evaluate
	\end{itemize}
	\paragraph{} For our dataset, we used the google/code\_x\_glue\_ct\_code\_to\_text dataset available here: https://huggingface.co/datasets/google/code\_x\_glue\_ct\_code\_to\_text. We made use of the RobertaTokenizer for preprocessing; specifically we began with the pre-trained Salesforce/codet5-small model for some starting weights. Due to time constraints, we decided to skip the examples in the dataset so as to make our training time feasible on our hardware. To pad tokens, we used a value of -100 for our labels so as to avoid reduced model performance. 
	\paragraph{} We selected a batch size of 16 for our training dataset, and a batch size of 8 for both our validation and testing datasets so as to avoid running out of memory on our selected hardware. We then used the T5 model from the Hugging Face Transformers library to train our model on Colorado State University's computer science department machines. We decided to go with Roberta Tokenizer since it creates byte-level Byte-Pair-Encoding, suitable for our base model. Additionally, we used the accelerate library to speed up training. Finally, we used the evaluate library to evaluate our model.
	\paragraph{} We unfortunately found that higher values used as hyper parameters, particularly those used in the number of epochs, drastically increased our training time. With our limited time, we decided to move forward with a proof-of-concept using a number of epochs equal to one with zero warm-up steps being used. Using CUDA, we trained our model primarily on our available GPUs using the AdamW optimizer.

	\section{Results}
	\paragraph{} Even with only a single epoch, we were able to generate the following docstring: “Wrap an environment into a deepmind environment.” , with the True string being “ Configure environment for DeepMind-style Atari.” Even though it is not perfect, this is a promising result, as it shows that our model is able to generate a docstring that is both accurate and concise. We believe that with more training, our model could generate even more accurate and useful docstrings. Using a simple Evaluation: Score = (Number of overlapping words)/(Total Unique Words in Both strings) the score is 0.09 or 9\%. Using Bleu evaluation, geometric mean of modified precision scores multiplied by a brevity penalty(Penalizes short candidate sentences) we are getting a 0.033 or a 3.3\% and using rouge (Recall-Oriented Understudy for Gisting) Evaluation, which is focuses on recall, precision, and F-measure of n-gram overlaps, as well as longest common subsequences(rougeL): (rouge1, .285), (rouge2, 0), (rougeL, .285).

	\section{Discussion}
	\paragraph{} Our model scores horribly compared to other general models such as ChatGPT-4 with a BLEU score of 0.88 and MateCat with a BLEU score of 0.82. \cite{ghassemiazghandi2024evaluation} This is to be expected, however, as our model was trained with a very limited number of epochs on a smaller dataset. It is interesting to observe that we still got a competent docstring that, while not fully accurate, still conveys the general idea of the code. This is a promising result, as it shows that even with limited resources, it is possible to generate useful docstrings. With an astoundingly low training time of just a few hours on weak hardware, models like ours could be used on code for quick and dirty documentation where complexity and maintainability is not a concern.

	\section{Conclusion}
	\paragraph{} The results of this project showed that it's possible to use machine learning to generate Python documentation, even with limited resources. While our model's scores were far from perfect and didn't match up to more advanced models like ChatGPT-4, it still managed to produce docstrings that were relevant and on-topic. This proves the concept works, with more training time and optimized parameters there's a lot of potential for improving the results. In the future, models like this could play a big role in making the process of creating software documentation faster and more efficient.

	\section{Future Work}
	\paragraph{} Our greatest challenges with this experiment were definitely a lack of computational resources and time. We would like to explore the potential of using a larger dataset, as well as training for more epochs. We would also like to explore the potential of using a larger model, such as the Salesforce/codet5-large model. We would also like to explore the potential of using a different optimizers, such as the SGD optimizer. With more time and computational power, we might be able to find a convergence on a more optimal solution using SGD. We might also try SGD with momentum if we still find ourselves limited on resources in the future to help reach a balance between optimal convergence and resource usage.
	\paragraph{} We would also like to perform more tests and analysis on our model to better understand its performance and potential improvements. This would help us compare our model to other works and evaluate where our model needs improvement, as well as where it really excels. Perhaps performing this analysis would help us and other researchers develop better and better models for the purpose of intelligent Python documentation generation.

	\pagebreak
	\bibliographystyle{plain}
	\bibliography{refs}

\end{document}



