%        File: report.tex
%     Created: Tue Nov 12 05:00 PM 2024 M
% Last Change: Tue Nov 12 05:00 PM 2024 M
%

\documentclass[12pt]{article}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{amsmath}%
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{graphicx}
\usepackage{enumitem}
%-------------------------------------------
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\setlength{\textwidth}{7.0in}
\setlength{\oddsidemargin}{-0.35in}
\setlength{\topmargin}{-0.5in}
\setlength{\textheight}{9.0in}
\setlength{\parindent}{0.3in}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\U}{\mathbb{U}}

\usepackage{listings}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
backgroundcolor=\color{backcolour},   
commentstyle=\color{codegreen},
keywordstyle=\color{magenta},
numberstyle=\tiny\color{codegray},
stringstyle=\color{codepurple},
basicstyle=\ttfamily\footnotesize,
breakatwhitespace=false,         
breaklines=true,                 
captionpos=b,                    
keepspaces=true,                 
numbers=left,                    
numbersep=5pt,                  
showspaces=false,                
showstringspaces=false,
showtabs=false,                  
tabsize=2
}

\lstset{style=mystyle}

\begin{document}
	\title{ AI-Generated Python Code Documentation: A Case Study }
	\author{Levi Shafter, Ryan Martel, Kevin Zhu}
	\maketitle

	\section*{Abstract}
	<++>

	\section{Introduction}
	\paragraph{} In an era of LLM-based revolution, many are exploring the endless possibilities of using artificial intelligence to aid in software development. This comes as no surprise considering the astounding need for software in nearly every aspect of modern industry. \cite{emerald2019csrImpact} With a dramatic increase in the need for software, some of the hottest topics being explored are the potential of enhancing the software development process, creating new tools as well as enhancing the old, and even automating certain aspects of writing code (See Section 3: Related Work). 
	\paragraph{} In this paper, we detail our experience with training a model using machine learning for the purpose of generating Python documentation. We discuss other similar works, compare our results, and discuss applications and potential improvements as well as viable future work. With so much recent progress in the field of AI \cite{shietal2020IoT_AI}, models like ours might drastically change the way we think about and approach software development.

	\section{Related Work}
	\paragraph{} Several recent studies and developments in the field of artificial intelligence and software development have highlighted significant advancements. These include the use of ML in resolving code review comments \cite{google2023codeReviewML}, the introduction of IDE plugins for AI-assisted development \cite{qodoAI2024idePlugin}, innovations in AI-powered fuzzing \cite{google2023aiFuzzing, cso2024aiFuzzing}, research into the out-of-distribution generalization of pre-trained language models \cite{chen2023oodGeneralization}, efforts in automated documentation with ChatGPT \cite{awekrx2024autodoc}, and the enhancement of large language models for multi-modal research synthesis \cite{wiggers2023largeModels}.


	\section{Method}
	For building our model, we used the following Python modules:
	\begin{itemize}
		\item datasets
		\item torch
		\item transformers
		\item accelerate
		\item evaluate
	\end{itemize}
	\paragraph{} For our dataset, we used the google/code\_x\_glue\_ct\_code\_to\_text dataset available here: https://huggingface.co/datasets/google/code\_x\_glue\_ct\_code\_to\_text. We made use of the RobertaTokenizer for preprocessing; specifically we began with the pretrained Salesforce/codet5-small model for some starting weights. Due to time constraints, we decided to skip the examples in the dataset so as to make our training time feasible on our hardware. To pad tokens, we used a value of -100 for our labels so as to avoid reduced model performance. 
	\paragraph{} We selected a batch size of 16 for our training dataset, and a batch size of 8 for both our validation and testing datasets so as to avoid running out of memory on our selected hardware. We then used the T5 model from the Hugging Face Transformers library to train our model on Colorado State University's computer science department machines. We decided to go with Roberta Tokenizer since it creates byte-level Byte-Pair-Encoding, suitable for our base model. Additionally, we used the accelerate library to speed up training. Finally, we used the evaluate library to evaluate our model.
	\paragraph{} We unfortunately found that higher values used as hyper parameters, particularly those used in the number of epochs, drastically increased our training time. With our limited time, we decided to move forward with a proof-of-concept using a number of epochs equal to one with zero warmup steps being used. Using CUDA, we trained our model primarily on our available GPUs using the AdamW optimizer.

	\section{Results}
	\paragraph{} Even with only a single epoch, we were able to generage the following docstring: ``Wrap an environment into a deepmind environment.'' This is a promising result, as it shows that our model is able to generate a docstring that is both accurate and concise. We believe that with more training, our model could generate even more accurate and useful docstrings.

	\section{Discussion}
	\paragraph{} Our model scores horribly compared to other general models such as ChatGPT-4 with a BLEU score of 0.88 and MateCat with a BLEU score of 0.82. \cite{ghassemiazghandi2024evaluation} This is to be expected, however, as our model was trained with a very limited number of epochs on a smaller dataset. It is interesteing to observe that we still got a competent docstring that, while not fully accurate, still conveys the general idea of the code. 

	\section{Conclusion}
	<++>

	\section{Future Work}
	\paragraph{} Our greatest challenges with this experiment were definitely a lack of computational resources and time. We would like to explore the potential of using a larger dataset, as well as training for more epochs. We would also like to explore the potential of using a larger model, such as the Salesforce/codet5-large model. We would also like to explore the potential of using a different optimizers, such as the SGD optimizer. With more time and computational power, we might be able to find a convergence on a more optimal solution using SGD. We might also try SGD with momentum if we still find ourselves limited on resources in the future to help reach a balance between optimal convergence and resource usage.
	\paragraph{} We would also like to perform more tests and analysis on our model to better understand its performance and potential improvements. This would help us compare our model to other works and evaluate where our model needs improvement, as well as where it really excels. Perhaps performing this analysis would help us and other researchers develop better and better models for the purpose of intelligent Python documentation generation.

	\pagebreak
	\bibliographystyle{plain}
	\bibliography{refs}

\end{document}



